{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Video Dataset of Human Demonstrations of Folding Clothing For Robotic Folding Andreas Verleysen, Matthijs Biondina and Francis wyffels IDLab-AIRO, Ghent University-imec Radbout University, Netherlands Introduction General-purpose cloth folding robots do not yet exist due to the deformable nature of textile, making it hard to engineer manipulation pipelines or learn this task. In order to accelerate research for the learning of the robotic folding task, we introduce a video dataset of human folding demonstrations. In total, we provide 8.5 hours of demonstrations from multiple perspectives leading to 1.000 folding samples of different types of textiles. The demonstrations are recorded on multiple public places, in different conditions with random people. Our dataset consists of anonymized RGB images, depth frames, skeleton keypoint trajectories, and object labels. Features The folding demonstrations in this dataset are exposed using the following features: RGB frames Depth values Skeleton keypoint trajectories Subtask labeling Descriptive labels such as folding method and type of textile being folded Downloads The following downloads are available: Paper Dataset - 5% sample (3.2 GB) Dataset - 20% sample (132 GB) Dataset - complete : this link is upcoming Code Directory hierarchy The samples are split and organized per folding demonstration of one piece of textile.","title":"Home"},{"location":"#video-dataset-of-human-demonstrations-of-folding-clothing-for-robotic-folding","text":"Andreas Verleysen, Matthijs Biondina and Francis wyffels IDLab-AIRO, Ghent University-imec Radbout University, Netherlands","title":"Video Dataset of Human Demonstrations of Folding Clothing For Robotic Folding"},{"location":"#introduction","text":"General-purpose cloth folding robots do not yet exist due to the deformable nature of textile, making it hard to engineer manipulation pipelines or learn this task. In order to accelerate research for the learning of the robotic folding task, we introduce a video dataset of human folding demonstrations. In total, we provide 8.5 hours of demonstrations from multiple perspectives leading to 1.000 folding samples of different types of textiles. The demonstrations are recorded on multiple public places, in different conditions with random people. Our dataset consists of anonymized RGB images, depth frames, skeleton keypoint trajectories, and object labels.","title":"Introduction"},{"location":"#features","text":"The folding demonstrations in this dataset are exposed using the following features: RGB frames Depth values Skeleton keypoint trajectories Subtask labeling Descriptive labels such as folding method and type of textile being folded","title":"Features"},{"location":"#downloads","text":"The following downloads are available: Paper Dataset - 5% sample (3.2 GB) Dataset - 20% sample (132 GB) Dataset - complete : this link is upcoming Code","title":"Downloads"},{"location":"#directory-hierarchy","text":"The samples are split and organized per folding demonstration of one piece of textile.","title":"Directory hierarchy"},{"location":"about/","text":"About AIRO The authors are affiliated with IDLab AIRO, imec at Ghent University. Authors Andreas Verleysen Andreas is a candidate PhD student at Ghent university. He researches how robots can learn to manipulate highly deformable objects in a scaffolded way by observing human examples. Matthijs Biondina Bachelor Artificial Intelligence Radboud University Nijmegen, Master in Computing Science with specialization in Data Science at Radboud University Nijmegen. Currently specializing in robotics at Ghent University. Future candidate PhD student at Ghent University. Francis wyffels Francis wyffels (Ghent University - IDLab-AIRO) is an early career professor with expertise in the domain of machine learning and robotics. Additionally, he has expertise in designing smart products and works on unconventional computing. Apart from his research, he strongly believes in dissemination activities. By means of international projects with children, students, and schools, he wants to ensure a more realistic perspective on what robotics can achieve, and reduce the almost innate fear for robotics.","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#airo","text":"The authors are affiliated with IDLab AIRO, imec at Ghent University.","title":"AIRO"},{"location":"about/#authors","text":"","title":"Authors"},{"location":"about/#andreas-verleysen","text":"Andreas is a candidate PhD student at Ghent university. He researches how robots can learn to manipulate highly deformable objects in a scaffolded way by observing human examples.","title":"Andreas Verleysen"},{"location":"about/#matthijs-biondina","text":"Bachelor Artificial Intelligence Radboud University Nijmegen, Master in Computing Science with specialization in Data Science at Radboud University Nijmegen. Currently specializing in robotics at Ghent University. Future candidate PhD student at Ghent University.","title":"Matthijs Biondina"},{"location":"about/#francis-wyffels","text":"Francis wyffels (Ghent University - IDLab-AIRO) is an early career professor with expertise in the domain of machine learning and robotics. Additionally, he has expertise in designing smart products and works on unconventional computing. Apart from his research, he strongly believes in dissemination activities. By means of international projects with children, students, and schools, he wants to ensure a more realistic perspective on what robotics can achieve, and reduce the almost innate fear for robotics.","title":"Francis wyffels"},{"location":"examples/","text":"Code examples Code examples on how to use the folding_demonstrations API can be found here . Installation instructions are available in the readme of the git project. Scenario 1 - query RGB images from one perspective from folding_demonstrations.dataset import FoldingDemonstrationDataSet # Set to the directory where the folding demonstrations dataset is stored home_dir = '/media/data/folding_data_output' # Load the data dataset = FoldingDemonstrationDataSet(home_dir, perspectives: Tuple = ('left'), rgb=True, depth=False, pose=False, subtask=False, reward=False) # Iterate over data and query available information for demonstration in dataset: for frame in demonstration: rgb = frame['left']['rgb'] # Do something with the RGB image Scenario 2 - query wrist pose estimation from folding sub-task only from folding_demonstrations.dataset import FoldingDemonstrationDataSet # Set to the directory where the folding demonstrations dataset is stored home_dir = '/media/data/folding_data_output' # Load the data dataset = FoldingDemonstrationDataSet(home_dir, perspectives: Tuple = ('right'), rgb=False, depth=False, pose=True, subtask=True, reward=False) minimum_score = 0.5 # Iterate over data and query available information for demonstration in dataset: for frame in demonstration: if frame['subtask'] == 'folding': pose = frame['right']['pose'] if pose.confidence >= minimum_score: LWrist_xy = pose['LWrist'][0:2] RWrist_xy = pose['RWrist'][0:2] # Do something with the pose data","title":"Examples"},{"location":"examples/#code-examples","text":"Code examples on how to use the folding_demonstrations API can be found here . Installation instructions are available in the readme of the git project.","title":"Code examples"},{"location":"examples/#scenario-1-query-rgb-images-from-one-perspective","text":"from folding_demonstrations.dataset import FoldingDemonstrationDataSet # Set to the directory where the folding demonstrations dataset is stored home_dir = '/media/data/folding_data_output' # Load the data dataset = FoldingDemonstrationDataSet(home_dir, perspectives: Tuple = ('left'), rgb=True, depth=False, pose=False, subtask=False, reward=False) # Iterate over data and query available information for demonstration in dataset: for frame in demonstration: rgb = frame['left']['rgb'] # Do something with the RGB image","title":"Scenario 1 - query RGB images from one perspective"},{"location":"examples/#scenario-2-query-wrist-pose-estimation-from-folding-sub-task-only","text":"from folding_demonstrations.dataset import FoldingDemonstrationDataSet # Set to the directory where the folding demonstrations dataset is stored home_dir = '/media/data/folding_data_output' # Load the data dataset = FoldingDemonstrationDataSet(home_dir, perspectives: Tuple = ('right'), rgb=False, depth=False, pose=True, subtask=True, reward=False) minimum_score = 0.5 # Iterate over data and query available information for demonstration in dataset: for frame in demonstration: if frame['subtask'] == 'folding': pose = frame['right']['pose'] if pose.confidence >= minimum_score: LWrist_xy = pose['LWrist'][0:2] RWrist_xy = pose['RWrist'][0:2] # Do something with the pose data","title":"Scenario 2 - query wrist pose estimation from folding sub-task only"},{"location":"license/","text":"License This work is licensed under a Creative Commons Attribution 4.0 International License .","title":"License"},{"location":"license/#license","text":"This work is licensed under a Creative Commons Attribution 4.0 International License .","title":"License"},{"location":"setup/","text":"Setup Douglas - our folding table A folding table with cameras mounted on top was designed and constructed for the purpose of capturing video demonstrations. Types of textile There are three different types of textile in the dataset that has to be folded: towels, shirts and hoodies. Anonymization All RGB frames are anonymized by applying color quantization to the corners of the frame and on a patch around the face of the demonstrator. This hides facial features without compromising image fidelity.","title":"Setup"},{"location":"setup/#setup","text":"","title":"Setup"},{"location":"setup/#douglas-our-folding-table","text":"A folding table with cameras mounted on top was designed and constructed for the purpose of capturing video demonstrations.","title":"Douglas - our folding table"},{"location":"setup/#types-of-textile","text":"There are three different types of textile in the dataset that has to be folded: towels, shirts and hoodies.","title":"Types of textile"},{"location":"setup/#anonymization","text":"All RGB frames are anonymized by applying color quantization to the corners of the frame and on a patch around the face of the demonstrator. This hides facial features without compromising image fidelity.","title":"Anonymization"}]}